{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlining Walmart's Sales Data Analysis with Polars and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data analysis is an essential aspect of retail operations, especially for a company like Walmart with vast amounts of sales data. Managing, cleaning, and visualizing this data efficiently can provide invaluable insights. In this article, we'll explore how to streamline the process of handling Walmart's sales data using Python libraries such as Pandas, Polars, and Seaborn.\n",
    "\n",
    "We'll begin by importing the necessary libraries and loading the datasets. Polars, a high-performance DataFrame library, will be our main tool for data manipulation, while Pandas will be used for specific tasks where it excels. Seaborn and Matplotlib will help us create insightful visualizations.\n",
    "\n",
    "First, let's set up our environment and load the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30490, 1947)\n",
      "shape: (5, 1_947)\n",
      "┌────────────────┬───────────────┬─────────────┬───────────┬───┬────────┬────────┬────────┬────────┐\n",
      "│ id             ┆ item_id       ┆ dept_id     ┆ cat_id    ┆ … ┆ d_1938 ┆ d_1939 ┆ d_1940 ┆ d_1941 │\n",
      "│ ---            ┆ ---           ┆ ---         ┆ ---       ┆   ┆ ---    ┆ ---    ┆ ---    ┆ ---    │\n",
      "│ str            ┆ str           ┆ str         ┆ str       ┆   ┆ i64    ┆ i64    ┆ i64    ┆ i64    │\n",
      "╞════════════════╪═══════════════╪═════════════╪═══════════╪═══╪════════╪════════╪════════╪════════╡\n",
      "│ HOUSEHOLD_2_32 ┆ HOUSEHOLD_2_3 ┆ HOUSEHOLD_2 ┆ HOUSEHOLD ┆ … ┆ 0      ┆ 0      ┆ 0      ┆ 0      │\n",
      "│ 6_CA_4_evaluat ┆ 26            ┆             ┆           ┆   ┆        ┆        ┆        ┆        │\n",
      "│ io…            ┆               ┆             ┆           ┆   ┆        ┆        ┆        ┆        │\n",
      "│ FOODS_3_793_CA ┆ FOODS_3_793   ┆ FOODS_3     ┆ FOODS     ┆ … ┆ 3      ┆ 3      ┆ 4      ┆ 2      │\n",
      "│ _2_evaluation  ┆               ┆             ┆           ┆   ┆        ┆        ┆        ┆        │\n",
      "│ HOBBIES_1_363_ ┆ HOBBIES_1_363 ┆ HOBBIES_1   ┆ HOBBIES   ┆ … ┆ 0      ┆ 1      ┆ 0      ┆ 2      │\n",
      "│ WI_2_evaluatio ┆               ┆             ┆           ┆   ┆        ┆        ┆        ┆        │\n",
      "│ n              ┆               ┆             ┆           ┆   ┆        ┆        ┆        ┆        │\n",
      "│ FOODS_2_141_CA ┆ FOODS_2_141   ┆ FOODS_2     ┆ FOODS     ┆ … ┆ 0      ┆ 1      ┆ 0      ┆ 0      │\n",
      "│ _4_evaluation  ┆               ┆             ┆           ┆   ┆        ┆        ┆        ┆        │\n",
      "│ HOUSEHOLD_2_29 ┆ HOUSEHOLD_2_2 ┆ HOUSEHOLD_2 ┆ HOUSEHOLD ┆ … ┆ 0      ┆ 0      ┆ 0      ┆ 0      │\n",
      "│ 2_TX_3_evaluat ┆ 92            ┆             ┆           ┆   ┆        ┆        ┆        ┆        │\n",
      "│ io…            ┆               ┆             ┆           ┆   ┆        ┆        ┆        ┆        │\n",
      "└────────────────┴───────────────┴─────────────┴───────────┴───┴────────┴────────┴────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "# Define file paths\n",
    "path_sales = '/Users/fredrik.hornell/Python/Private/Walmart_M5/data/raw/sales_train_evaluation.parquet'\n",
    "path_calendar = '/Users/fredrik.hornell/Python/Private/Walmart_M5/data/raw/calendar.parquet'\n",
    "path_price = '/Users/fredrik.hornell/Python/Private/Walmart_M5/data/raw/sell_prices.parquet'\n",
    "\n",
    "# Load datasets\n",
    "df_sale = pl.read_parquet(path_sales)\n",
    "df_calendar = pl.read_parquet(path_calendar)\n",
    "df_prices = pl.read_parquet(path_price)\n",
    "\n",
    "# Display basic information\n",
    "print(df_sale.shape)\n",
    "print(df_sale.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data\n",
    "After loading the datasets, we need to understand the structure and contents of our sales data. This includes examining the various identifiers such as item_id, dept_id, cat_id, store_id, and state_id. Sampling these columns helps us get a sense of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ id                              │\n",
      "│ ---                             │\n",
      "│ str                             │\n",
      "╞═════════════════════════════════╡\n",
      "│ HOUSEHOLD_2_301_CA_4_evaluatio… │\n",
      "│ HOBBIES_2_066_TX_2_evaluation   │\n",
      "│ HOUSEHOLD_1_457_WI_2_evaluatio… │\n",
      "└─────────────────────────────────┘\n",
      "shape: (3, 1)\n",
      "┌─────────────────┐\n",
      "│ item_id         │\n",
      "│ ---             │\n",
      "│ str             │\n",
      "╞═════════════════╡\n",
      "│ HOBBIES_2_112   │\n",
      "│ FOODS_3_118     │\n",
      "│ HOUSEHOLD_1_022 │\n",
      "└─────────────────┘\n",
      "shape: (3, 1)\n",
      "┌─────────────┐\n",
      "│ dept_id     │\n",
      "│ ---         │\n",
      "│ str         │\n",
      "╞═════════════╡\n",
      "│ HOUSEHOLD_1 │\n",
      "│ HOUSEHOLD_2 │\n",
      "│ HOBBIES_1   │\n",
      "└─────────────┘\n",
      "shape: (3, 1)\n",
      "┌───────────┐\n",
      "│ cat_id    │\n",
      "│ ---       │\n",
      "│ str       │\n",
      "╞═══════════╡\n",
      "│ FOODS     │\n",
      "│ HOBBIES   │\n",
      "│ HOUSEHOLD │\n",
      "└───────────┘\n",
      "shape: (3, 1)\n",
      "┌──────────┐\n",
      "│ store_id │\n",
      "│ ---      │\n",
      "│ str      │\n",
      "╞══════════╡\n",
      "│ WI_3     │\n",
      "│ WI_3     │\n",
      "│ WI_1     │\n",
      "└──────────┘\n",
      "shape: (3, 1)\n",
      "┌──────────┐\n",
      "│ state_id │\n",
      "│ ---      │\n",
      "│ str      │\n",
      "╞══════════╡\n",
      "│ TX       │\n",
      "│ WI       │\n",
      "│ WI       │\n",
      "└──────────┘\n"
     ]
    }
   ],
   "source": [
    "print(df_sale.select(\"id\").sample(3))\n",
    "print(df_sale.select(\"item_id\").sample(3))\n",
    "print(df_sale.select(\"dept_id\").sample(3))\n",
    "print(df_sale.select(\"cat_id\").sample(3))\n",
    "print(df_sale.select(\"store_id\").sample(3))\n",
    "print(df_sale.select(\"state_id\").sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Data\n",
    "To analyze daily sales trends and product performance more effectively, we need to transform the dataset into a more suitable format. This transformation involves two main steps: creating a \"sales\" DataFrame for sales data and a \"product-location\" DataFrame for metadata. This division helps in streamlining the analysis process and making the data more manageable from a memory perspectiva, have in mind this is done on a lapttop with 8 gb ram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Sales DataFrame\n",
    "We start by creating a \"sales\" DataFrame that focuses on daily sales data. This step involves melting the original sales DataFrame to convert it from a wide format (with days as columns) to a long format (with days as rows), and joining it with the calendar data to include the date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_sale has shape (30490, 1947)\n",
      "df_sales has shape (59181090, 3)\n"
     ]
    }
   ],
   "source": [
    "df_sales = (\n",
    "    df_sale.lazy()\n",
    "    .drop(['item_id','dept_id','cat_id','store_id','state_id'])\n",
    "    .melt(id_vars='id')\n",
    "    .rename({'variable': 'day', 'value': 'sales'})\n",
    "    .with_columns(pl.col('sales').cast(pl.Int16))\n",
    "    .join(df_calendar.select([pl.col(\"date\"), pl.col(\"d\").alias(\"day\")]).lazy(), on=\"day\", how=\"inner\")\n",
    "    .drop(['day'])\n",
    "    .select(pl.col('id'), pl.col('date'), pl.col('sales'))\n",
    "    .collect()\n",
    ")\n",
    "print(f\"df_sale has shape {df_sale.shape}\\ndf_sales has shape {df_sales.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we do this?**\n",
    "\n",
    "1. **Simplifying Time Series Analysis:** By converting the data into a long format, each row represents a unique product-location combination for a specific day. This format simplifies the analysis of sales trends over time, making it easier to perform operations like aggregations, filtering, and time series analysis.\n",
    "\n",
    "2. **Including Date Information:** Joining with the calendar data allows us to include the actual date information in the DataFrame. This is essential for any time-based analysis, such as identifying trends, seasonality, and other temporal patterns.\n",
    "\n",
    "3. **Memory Efficiency:** The transformation to a long format significantly reduces memory usage when performing operations. Wide DataFrames with many columns can be less efficient in terms of memory, especially when dealing with missing or sparse data. A long format is more memory-efficient for large datasets, as it reduces redundancy and optimizes storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Product-Location Metadata DataFrame\n",
    "Next, we generate a \"product-location\" DataFrame that combines product and location metadata with sales data. This DataFrame includes important metadata such as item ID, department ID, category ID, store ID, and state ID. We also calculate metrics like the first and last transaction dates, total unit sales, and create a timeseries date range for each product-location combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vg/gffx6m9j07j37nkdpql4hwr00000gq/T/ipykernel_42817/3834817002.py:13: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  .collect()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30490, 11)\n"
     ]
    }
   ],
   "source": [
    "df_prodloc = (\n",
    "    df_sale.lazy()\n",
    "    .select(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])\n",
    "    .join(df_sales.lazy(), on=\"id\", how=\"inner\")\n",
    "    .group_by(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])\n",
    "    .agg([\n",
    "        (pl.col(\"date\").filter(pl.col(\"sales\") > 0)).min().alias(\"first_tran_dt\"),\n",
    "        (pl.col(\"date\").filter(pl.col(\"sales\") > 0)).max().alias(\"last_tran_dt\"),\n",
    "        pl.col(\"date\").max().alias(\"dataset_last_dt\"),\n",
    "        pl.col(\"sales\").sum().alias(\"total_unit_sales\")\n",
    "    ])\n",
    "    .with_columns(pl.col('first_tran_dt').map_elements(lambda x: pl.date_range(x, date(2016, 5, 22), interval=\"1d\", eager=True)).alias('timeseries_daterange'))\n",
    "    .collect()\n",
    ")\n",
    "print(df_prodloc.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we do this?**\n",
    "\n",
    "1. **Aggregating Metadata:** This DataFrame aggregates important metadata and sales metrics for each product-location combination. It provides a summary of each product's performance in different locations, including the first and last transaction dates and total sales.\n",
    "\n",
    "2. **Facilitating Detailed Analysis:** By separating the metadata from the daily sales data, we can perform more detailed and targeted analysis. For instance, we can use this DataFrame to study sales patterns across different product categories, departments, and regions.\n",
    "\n",
    "3. **Memory Efficiency:** Storing metadata separately from the sales data helps reduce memory usage. Metadata tends to be less volatile and smaller in size compared to daily transactional data. By separating these concerns, we can load and process large volumes of sales data more efficiently without overwhelming memory resources.\n",
    "\n",
    "4. **Creating a Complete Time Series:** The timeseries_daterange column ensures that we have a complete date range for each product-location combination, even if no sales were recorded on some days. This completeness is crucial for accurate time series analysis and forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Adjusted Sales DataFrame\n",
    "To ensure each product-location combination has a complete and accurate time series of sales data, we create an adjusted dataset df_sale_adj. This step involves generating a row for every date within the specified range and ensuring that periods before a product's introduction (where no sales occurred) are excluded. This helps in creating a realistic view of the product's sales lifecycle. This leads to ca 20 % reduction in rows in the sales datafram \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59181090, 3)\n",
      "(46796220, 3)\n"
     ]
    }
   ],
   "source": [
    "df_sales_adj = (\n",
    "    df_prodloc.lazy()\n",
    "    .select([pl.col('id'), pl.col('timeseries_daterange').alias('date')])\n",
    "    .explode('date')  # Ensures every date within the range is included\n",
    "    .join(df_sales.lazy(), on=['id', 'date'], how='inner')  # Join with sales data\n",
    "    .collect()  # Collect the results into a DataFrame\n",
    ")\n",
    "\n",
    "print(df_sales.shape)\n",
    "print(df_sales_adj.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Transforming Walmart's sales data into structured formats significantly enhances our ability to analyze and derive insights efficiently. Here's an overview of the process and its benefits:\n",
    "\n",
    "Firstly, we restructure the original sales data into a \"sales\" DataFrame (*df_sales* & *df_sales_adj*). This transformation involves converting the data into a long format and incorporating crucial date information. By simplifying the data structure and optimizing memory usage, we facilitate streamline data handling.\n",
    "\n",
    "Additionally, we create a \"Product-Location Metadata\" DataFrame (df_prodloc) to consolidate essential metadata and sales metrics for each product-location combination. This aggregation enables detailed analysis across product categories, departments, and regions while enhancing memory efficiency by separating metadata from transactional data.\n",
    "\n",
    "Lastly, to ensure data integrity and efficient future access, we save these transformed DataFrames as parquet files (df_sales.parquet, df_sales_adj.parquet, df_prodloc.parquet). This approach not only supports scalable data management but also ensures that our analyses are both insightful and resource-efficient.\n",
    "\n",
    "In summary, restructuring Walmart's sales data into these optimized formats empowers us to uncover meaningful insights.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fredrik_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
